{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b37f4ec",
   "metadata": {},
   "source": [
    "**Create Matrix A, X, Eta,B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5727c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "A = [[0] * 500] * 1000\n",
    "for row in range(1000):\n",
    "    for col in range(500):\n",
    "        A[row][col] = np.random.uniform(-1, 1)\n",
    "\n",
    "X = [0] * 500\n",
    "for i in range(500):\n",
    "    X[i] = np.random.uniform(-1, 1)\n",
    " \n",
    "sd = math.pow(0.5, 0.5)\n",
    "E= np.random.normal(0, sd, 1000)\n",
    "\n",
    "\n",
    "B = np.add(np.matmul(A, X),E)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf58335",
   "metadata": {},
   "source": [
    "**Loss Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5254f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a, w, b):\n",
    "    final_loss = 0\n",
    "    temp= np.matmul(a, w)-b\n",
    "    final_loss = np.matmul(np.transpose(temp), temp)\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3a979",
   "metadata": {},
   "source": [
    "**Implementing Gardiant Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c67161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, B,learning_rate, iterations):\n",
    "    w = [0]*500\n",
    "    print(\"Starting Loss:\", loss(A, w, B))\n",
    "    for i in range(iterations):\n",
    "        w = w -learning_rate * gradient(A, w, B)\n",
    "        print(\"loss at \"+str(i)+\"th iteration \" +str(loss(A, w, B)))\n",
    "    return w\n",
    "\n",
    "\n",
    "def gradient(A, w, B):\n",
    "    grad = 2 * (np.matmul(np.matmul(np.transpose(A), A),w)-np.matmul(np.transpose(A), B))\n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a136767",
   "metadata": {},
   "source": [
    "**Calling Gradient Descent , Finding distance , Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b977bc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Loss: 9486.390772817993\n",
      "loss at 0th iteration 4845.928518540698\n",
      "loss at 1th iteration 2597.4319894455903\n",
      "loss at 2th iteration 1507.9421144460287\n",
      "loss at 3th iteration 980.0390590633931\n",
      "loss at 4th iteration 724.2481221513503\n",
      "loss at 5th iteration 600.3067985707992\n",
      "loss at 6th iteration 540.2520840592332\n",
      "loss at 7th iteration 511.15308276672414\n",
      "loss at 8th iteration 497.0534091089903\n",
      "loss at 9th iteration 490.2215321226911\n",
      "loss at 10th iteration 486.9112042180218\n",
      "loss at 11th iteration 485.3072129783656\n",
      "loss at 12th iteration 484.5300126613624\n",
      "loss at 13th iteration 484.1534268560189\n",
      "loss at 14th iteration 483.97095540601606\n",
      "loss at 15th iteration 483.88254041640766\n",
      "loss at 16th iteration 483.83969968495427\n",
      "loss at 17th iteration 483.8189415727882\n",
      "loss at 18th iteration 483.8088834060371\n",
      "loss at 19th iteration 483.8040098068589\n",
      "loss at 20th iteration 483.8016483458155\n",
      "loss at 21th iteration 483.8005041199457\n",
      "loss at 22th iteration 483.7999496950294\n",
      "loss at 23th iteration 483.79968105315027\n",
      "loss at 24th iteration 483.7995508850119\n",
      "loss at 25th iteration 483.79948781314727\n",
      "loss at 26th iteration 483.7994572522115\n",
      "loss at 27th iteration 483.79944244417004\n",
      "loss at 28th iteration 483.7994352690593\n",
      "loss at 29th iteration 483.7994317924205\n",
      "loss at 30th iteration 483.7994301078447\n",
      "loss at 31th iteration 483.7994292915978\n",
      "loss at 32th iteration 483.79942889609237\n",
      "loss at 33th iteration 483.79942870445353\n",
      "loss at 34th iteration 483.79942861159657\n",
      "loss at 35th iteration 483.79942856660347\n",
      "loss at 36th iteration 483.7994285448025\n",
      "loss at 37th iteration 483.79942853423904\n",
      "loss at 38th iteration 483.7994285291205\n",
      "loss at 39th iteration 483.79942852664044\n",
      "loss at 40th iteration 483.7994285254387\n",
      "loss at 41th iteration 483.79942852485647\n",
      "loss at 42th iteration 483.7994285245743\n",
      "loss at 43th iteration 483.79942852443764\n",
      "loss at 44th iteration 483.79942852437136\n",
      "loss at 45th iteration 483.7994285243393\n",
      "loss at 46th iteration 483.79942852432373\n",
      "loss at 47th iteration 483.79942852431617\n",
      "loss at 48th iteration 483.79942852431253\n",
      "loss at 49th iteration 483.79942852431077\n"
     ]
    }
   ],
   "source": [
    "GD_start = time.time()\n",
    "Final_Weights= gradient_descent(A, B, 0.000001, 50)\n",
    "GD_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25f0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights Matrix:\n",
      "[-1.22185995e-02  1.82789021e-02 -1.27110873e-03  5.65557575e-03\n",
      "  1.44905522e-02  9.14305893e-03  1.10329087e-02  1.52718859e-02\n",
      " -1.13991157e-02  1.03909208e-03  8.86237605e-03 -8.05666573e-03\n",
      " -2.81467278e-03  4.19630212e-04  1.31536810e-02  7.04138350e-03\n",
      " -1.66401678e-02 -7.65585029e-03  5.50185943e-03 -4.60152244e-03\n",
      " -1.15397916e-03  7.25366178e-03 -1.16848306e-02 -9.27344370e-03\n",
      " -1.74674694e-02 -7.76091762e-03 -1.03391392e-02  1.86988372e-02\n",
      " -2.41677197e-03  4.96945496e-03  1.10227287e-02  1.28463103e-02\n",
      " -1.91835636e-02 -1.12576991e-02 -4.50800299e-03  1.63700526e-02\n",
      "  1.51820973e-02 -1.36839044e-02  1.20201553e-02 -9.36001585e-03\n",
      " -6.98954236e-03  3.63936265e-03  1.75125972e-02 -1.62643796e-02\n",
      "  5.99493555e-03 -1.64834140e-02  1.44924087e-02  1.61472698e-02\n",
      "  8.79617239e-04 -2.22718378e-03  4.61391536e-03 -2.70624465e-03\n",
      "  6.29546153e-04  1.13152133e-02  1.18149473e-02 -1.85573342e-02\n",
      " -1.20515965e-02 -1.37868179e-03  1.91941919e-02 -1.15643269e-02\n",
      " -7.07981567e-03 -1.53200545e-02 -1.38357548e-03  8.36924386e-03\n",
      "  1.87729462e-02 -1.42622501e-02  8.98744709e-03 -8.45583654e-03\n",
      " -2.76390129e-03  4.29010365e-03 -1.60975979e-02 -1.69964620e-03\n",
      " -8.72014736e-03 -2.47143948e-03  6.16124075e-03  1.36491107e-02\n",
      "  1.44280232e-02 -1.45848306e-02  6.35965216e-03  3.95721156e-03\n",
      " -1.24733628e-02  1.14678260e-02 -5.09960346e-03 -6.62076777e-03\n",
      "  5.57273064e-03  1.58361972e-03 -1.73197551e-02 -1.13279763e-02\n",
      " -1.97450055e-02 -1.42433229e-02 -1.47882232e-02 -2.57469571e-03\n",
      "  8.92472106e-03  1.18719204e-02 -9.73364308e-03  5.74442824e-03\n",
      " -1.18041665e-02 -2.48560585e-03 -2.90283704e-03  3.33143966e-03\n",
      " -7.66613459e-03 -2.52493411e-03  1.91405693e-02  4.60535789e-04\n",
      " -3.45266769e-03 -1.95161766e-02  1.27256561e-02 -9.97199792e-03\n",
      "  8.26180995e-04  3.31865269e-03 -4.96589896e-03 -9.06879440e-03\n",
      " -8.77025147e-03 -7.02549019e-03  1.44975701e-02  1.16590356e-02\n",
      "  5.73830248e-03  1.41919886e-02 -4.51237444e-03 -2.37602805e-03\n",
      " -1.56423902e-03  1.11452524e-02  6.83503579e-03  9.59127374e-03\n",
      "  1.86692923e-02 -1.35444287e-02  1.30607785e-02 -1.12363844e-02\n",
      " -9.10657748e-03  1.20952895e-02  1.25895348e-02  1.95221943e-02\n",
      "  3.84608844e-03 -1.79802081e-02  3.34029613e-03  6.39707273e-03\n",
      " -8.17505000e-03 -1.80306358e-02  1.82110754e-02  4.78111908e-03\n",
      " -2.14020269e-03  6.37679185e-03  9.26275197e-03  1.57906384e-02\n",
      " -8.48928020e-03 -6.12647426e-03  2.11866118e-03 -1.87233374e-03\n",
      " -1.76429818e-02  3.53591764e-05 -1.05747258e-02 -7.97296001e-03\n",
      " -1.06020910e-02 -4.25445921e-03 -3.21184920e-03 -4.68802661e-03\n",
      " -1.22595477e-02  1.42018767e-02 -1.01270439e-02  5.47112430e-03\n",
      " -1.51201945e-03 -5.01598919e-03 -1.20133621e-02  1.86825455e-02\n",
      " -1.14100796e-02  3.92233652e-04  1.38094471e-02 -6.04428142e-03\n",
      " -1.96162382e-02  1.80231127e-02  1.52739518e-02  1.48576012e-02\n",
      " -1.56447140e-02 -1.27140355e-02 -1.03204976e-02 -3.93264570e-04\n",
      "  8.13384397e-04 -1.92167347e-02  2.62529056e-03  3.76312528e-04\n",
      "  9.31816431e-03  1.45383617e-02  1.66958921e-02 -1.65944613e-02\n",
      " -4.73931558e-03  9.42436075e-03 -1.72192821e-02  1.10262637e-02\n",
      " -9.53136604e-03 -1.96170788e-02 -3.68800593e-03 -1.44510365e-02\n",
      " -7.61699686e-03 -4.29502177e-03  1.05120014e-03 -1.35950505e-02\n",
      " -1.68689110e-02 -1.15661015e-02 -1.24370763e-02  1.70362010e-02\n",
      "  4.02689457e-03  6.41054348e-03 -7.71979904e-03 -7.77464207e-04\n",
      " -1.96213009e-02 -1.11960506e-02 -1.11942797e-02  2.94822979e-04\n",
      " -3.51356598e-03 -2.01154095e-03 -1.71441947e-02  1.89924506e-02\n",
      " -1.45521508e-02  1.14234674e-02  1.65446861e-02 -8.04708740e-03\n",
      "  2.28166520e-03 -9.42794605e-03  1.29926446e-02  2.74714620e-03\n",
      " -9.24496300e-03 -1.70104701e-02  1.75506797e-03 -1.70408069e-02\n",
      "  1.38764893e-02 -1.02484655e-02 -5.33038905e-03 -5.62236817e-04\n",
      "  8.78400593e-03  1.64507415e-02 -1.34809260e-02 -1.89805978e-02\n",
      "  5.46730595e-03  6.62707576e-03 -5.04114215e-03 -1.65786643e-03\n",
      "  5.59386023e-03 -1.28357933e-02  3.64174402e-03  7.94518721e-03\n",
      " -9.66600735e-03  1.10924273e-02 -8.97392063e-03 -1.76479553e-02\n",
      "  3.57402631e-03  1.10341062e-02 -9.51285534e-03  4.28856355e-03\n",
      "  1.73581267e-02  4.85544518e-03 -2.55166784e-03  1.63539502e-02\n",
      "  1.19265079e-02 -4.56948001e-03 -1.11184945e-03  6.39840516e-04\n",
      "  8.11222761e-03 -1.12103664e-02 -3.34814658e-03 -7.67375747e-03\n",
      "  1.12456178e-04  6.60504082e-03  1.14133847e-02 -1.43538023e-02\n",
      "  1.09860264e-02 -1.80288246e-02 -1.55927632e-02 -2.76151264e-03\n",
      "  1.27267488e-02 -7.03255899e-03 -1.10787888e-02 -4.41363123e-03\n",
      " -2.20307526e-03  3.22016102e-03 -1.82556859e-03 -2.28741990e-04\n",
      "  1.06840462e-02  1.78652004e-02 -5.97116354e-03 -1.17973031e-02\n",
      " -1.01225997e-02 -1.19010789e-02  7.46975267e-03 -9.46557753e-03\n",
      " -3.53703949e-03 -1.64850749e-02 -1.38858718e-02 -1.79111738e-02\n",
      " -1.51409078e-02 -9.55148700e-03 -9.11392794e-03  1.60457707e-02\n",
      " -5.37855059e-03  9.77676338e-04  2.06697180e-03 -1.27748194e-02\n",
      " -4.04103329e-03 -7.06631551e-03  7.04699740e-03 -1.11763345e-02\n",
      " -4.96388997e-03  1.82104596e-02 -1.66334550e-03  4.45596358e-03\n",
      "  1.31920362e-02 -6.43898351e-03 -8.13098378e-04  1.65705270e-02\n",
      " -6.75943566e-03 -6.02072375e-03  7.57530516e-03 -1.50255945e-02\n",
      "  4.56195279e-03  2.59993047e-03 -1.71144919e-02  1.54272512e-02\n",
      " -6.64846596e-03  1.07687567e-02 -2.14301213e-04  1.56322959e-02\n",
      " -1.80908369e-02 -1.90455148e-02  1.96840126e-02  7.49595934e-03\n",
      " -4.72214257e-03  4.05747466e-03 -4.90554649e-03  2.17228837e-04\n",
      "  1.83069226e-02  3.27461067e-03 -7.66898994e-03  5.32171664e-03\n",
      " -1.97173679e-02 -1.46762274e-02  3.71585214e-03  1.51768971e-02\n",
      " -1.23765288e-02  1.78760154e-02 -1.12889840e-02  1.70814525e-03\n",
      "  1.24134412e-02 -1.07756778e-02 -6.43763042e-03 -1.64513991e-02\n",
      " -5.39456518e-03  8.67157716e-03  1.60207950e-02 -3.36784873e-03\n",
      "  1.53036099e-02  1.38173033e-03  5.74703622e-03  1.00365352e-02\n",
      "  1.68064066e-03  1.88244554e-03 -1.50977559e-02 -1.82481421e-03\n",
      " -3.54157152e-03  9.99199815e-03 -1.48189837e-02 -1.38462588e-02\n",
      " -1.65603902e-02 -4.68797889e-03  2.43758733e-03  1.86461044e-02\n",
      "  9.88198773e-03  3.99591374e-03  5.85151325e-04  1.42577442e-02\n",
      "  1.10678894e-03 -3.09260287e-03  9.91155798e-03 -6.45935338e-03\n",
      "  7.39577082e-03 -1.88308436e-02 -5.94261517e-03  1.12917845e-02\n",
      " -1.76314888e-02 -8.90177160e-03  8.48182613e-03 -1.94079942e-03\n",
      "  1.43624582e-02 -1.31802719e-02 -2.49464122e-03  1.96162445e-02\n",
      "  7.23179792e-03  1.71166453e-02 -5.86125244e-03  1.85893985e-02\n",
      "  1.20258647e-02  2.31426052e-03 -7.04022967e-03 -3.00778845e-03\n",
      "  6.66594822e-04 -1.83268858e-02 -3.17346959e-03  2.06825498e-03\n",
      "  8.37647770e-03 -1.06725491e-02  1.95377314e-02 -1.49032252e-02\n",
      "  6.32789215e-03  1.68774271e-02  5.76855326e-04  4.59463828e-03\n",
      " -1.95908081e-02  1.41705058e-02  1.75126308e-02 -4.40979977e-03\n",
      " -9.58539551e-03  5.93754705e-03  1.77282843e-02 -1.90818896e-03\n",
      " -1.19390566e-02 -1.85557738e-02  9.08108919e-03 -1.27270948e-03\n",
      " -1.62579992e-02 -1.00432863e-02  1.46093867e-02  1.94729952e-02\n",
      "  5.82262459e-03  6.38029606e-03  1.50526728e-02  5.68366918e-03\n",
      " -8.51684656e-03 -4.04506829e-03 -1.05105653e-02  3.61200189e-03\n",
      "  1.19585753e-02 -1.30437034e-02 -1.71847911e-02 -5.26396065e-03\n",
      "  2.60074461e-03  1.07944965e-02  5.11444641e-03  1.31318014e-02\n",
      "  1.96695014e-02 -1.16865970e-03  9.25280148e-04 -1.59652696e-02\n",
      "  3.10884117e-03 -1.42062663e-04  1.30331919e-02  1.76289562e-03\n",
      " -1.02497790e-02  1.01312595e-03 -9.22540817e-03 -3.72006923e-03\n",
      " -6.10943050e-03  2.89486572e-03 -6.12329348e-03  1.85894497e-02\n",
      "  1.68747589e-02  1.04991546e-02  1.56493728e-02  6.82251222e-03\n",
      "  5.31033297e-03  3.21698611e-03  5.61943739e-03  1.79033873e-02\n",
      "  1.28336483e-02 -5.49333270e-03  3.36634232e-03 -1.81388120e-02\n",
      "  1.19212685e-02 -1.12841077e-02 -8.81408445e-03 -1.02305284e-02\n",
      "  1.49402220e-02  7.72886811e-03  7.28432418e-03 -1.84421223e-03\n",
      "  7.98016567e-04  1.17101064e-02 -5.12349834e-03 -1.32506669e-02\n",
      "  1.04555760e-02 -1.02528121e-02  1.91726974e-02 -1.70010506e-02\n",
      "  1.04434102e-02  1.28474916e-02  1.15166050e-02 -3.70537200e-03\n",
      " -2.91812938e-03  3.02732317e-03  3.72636140e-03  1.16796265e-02\n",
      " -8.24637013e-03  1.54356053e-02 -1.48177094e-03 -1.11044841e-02\n",
      "  1.42831476e-02  4.47133550e-03 -1.35718494e-03 -1.47965041e-02\n",
      " -3.10690706e-03 -9.68220854e-03  3.37861149e-03 -5.53831218e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Weights Matrix:\")\n",
    "print(Final_Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d242df99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for gradient descent 5.80753493309021\n",
      "Distance to the hidden X taken: 159.55118166228192\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken for gradient descent\",GD_end-GD_start)\n",
    "temp=X-Final_Weights\n",
    "dist=np.matmul(np.transpose(temp),temp)\n",
    "print(\"Distance to the hidden X taken:\",dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b07932",
   "metadata": {},
   "source": [
    "**Comparision with Weights matrix that is found using the Regression Formula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2dac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg_start = time.time()\n",
    "Final_reg_weights=np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(A),A)),np.transpose(A)),B)\n",
    "Reg_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c70bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Regression Formula: 0.10427212715148926\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken for Regression Formula:\",Reg_end-Reg_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048f5f3",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da672ccf",
   "metadata": {},
   "source": [
    "<br>1. The Function Value after 50 steps =484\n",
    "<br>2. The distance to the Hidden X =160\n",
    "<br>3. The time taken with gradiant descent is 58 times the time taken using formuls.The ratio will increse more if number of iterations is increased in gradient descent.\n",
    "<br>\n",
    "<br> Here we had used the learning rate =0.000001 , beacuse if we take learning rate =0.1 and start running the gradient descent with intitial weight that is all zeros.Since the intial weight already very close to original X and if we calculate the gradient with learning rate =0.1 the algorithm pushing the new X that is really far and Function value is increasing . So with 0.1 learning rate the algorithm is not learning , instead producing high errors with oscillating between minimum.\n",
    "<br>By trying various learning rated i found that leaning_rate=0.000001 is optimal and reduced Function value really well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e049d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
