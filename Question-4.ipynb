{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f685fd9e",
   "metadata": {},
   "source": [
    "**Creating Data with n=100 since n value is not mentioned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d0e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data = []\n",
    "for i in range(100):\n",
    "    a = (i/100)\n",
    "    b = -1\n",
    "    point = (a, b)\n",
    "    data.append(point)\n",
    "for i in range(100):\n",
    "    a = (i/100)\n",
    "    b = 1\n",
    "    point = (a, b)\n",
    "    data.append(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28864745",
   "metadata": {},
   "source": [
    "**Calculating Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d451a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, data):\n",
    "    loss = 0\n",
    "    for i in range(len(data)):\n",
    "        loss += pow(x - data[i][0], 2) + pow(y - data[i][1], 2)\n",
    "    loss = loss / 400\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b2d4e",
   "metadata": {},
   "source": [
    "**Doing Gradient-Descent update at iteration t with \"eta\" vlaue seleteced based on algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9e4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, a, b, t):\n",
    "    #eta =0.1\n",
    "    #eta=0.1/(1 + t)\n",
    "    eta=0.1 / pow((1 + t), 0.5)\n",
    "    x_new = x * (1 - (2 * eta)) + 2 * eta * a\n",
    "    y_new = y * (1 - (2 * eta)) + 2 * eta * b\n",
    "    New_pair = (x_new, y_new)\n",
    "    return New_pair "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c748f",
   "metadata": {},
   "source": [
    "**Calling Gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f264de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, iterations):\n",
    "    x = 1\n",
    "    y = 1\n",
    "    print(\"Starting Loss:\", loss(x, y, data))\n",
    "    for i in range(iterations):\n",
    "        random_num = random.randint(0, 199)\n",
    "        a = data[random_num][0]\n",
    "        b = data[random_num][1]\n",
    "        grad = gradient(x, y, a, b, i)\n",
    "        x=grad[0]\n",
    "        y=grad[1]\n",
    "        print(\"loss at \"+str(i)+\"th iteration \" +str(loss(x, y, data)))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e451dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Loss: 1.169175\n",
      "loss at 0th iteration 1.103967\n",
      "loss at 1th iteration 0.8445339450522472\n",
      "loss at 2th iteration 0.8625063933826718\n",
      "loss at 3th iteration 0.73843138235149\n",
      "loss at 4th iteration 0.7483389842215975\n",
      "loss at 5th iteration 0.7596646105708156\n",
      "loss at 6th iteration 0.7715719183612657\n",
      "loss at 7th iteration 0.7859652346227783\n",
      "loss at 8th iteration 0.798245242839008\n",
      "loss at 9th iteration 0.7275341006417639\n",
      "loss at 10th iteration 0.7414131784494903\n",
      "loss at 11th iteration 0.6855929142612068\n",
      "loss at 12th iteration 0.6994888367973041\n",
      "loss at 13th iteration 0.7129206130361357\n",
      "loss at 14th iteration 0.7256869780535935\n",
      "loss at 15th iteration 0.6802016218204417\n",
      "loss at 16th iteration 0.6925735828328717\n",
      "loss at 17th iteration 0.6552935821234039\n",
      "loss at 18th iteration 0.6668148016554595\n",
      "loss at 19th iteration 0.6783067030115169\n",
      "loss at 20th iteration 0.6896477320084102\n",
      "loss at 21th iteration 0.6557961325698901\n",
      "loss at 22th iteration 0.6285808106466045\n",
      "loss at 23th iteration 0.6388384530267281\n",
      "loss at 24th iteration 0.6494614711720685\n",
      "loss at 25th iteration 0.6597291406545618\n",
      "loss at 26th iteration 0.6688590212301696\n",
      "loss at 27th iteration 0.6419455312819745\n",
      "loss at 28th iteration 0.6200216068699426\n",
      "loss at 29th iteration 0.6296156003321804\n",
      "loss at 30th iteration 0.6378710009624037\n",
      "loss at 31th iteration 0.6173334604316542\n",
      "loss at 32th iteration 0.5991471086434135\n",
      "loss at 33th iteration 0.6077512577222367\n",
      "loss at 34th iteration 0.6152588282122445\n",
      "loss at 35th iteration 0.624049359445017\n",
      "loss at 36th iteration 0.6317758876519678\n",
      "loss at 37th iteration 0.6401807432751448\n",
      "loss at 38th iteration 0.6201274307936516\n",
      "loss at 39th iteration 0.6284928655513814\n",
      "loss at 40th iteration 0.611425352500753\n",
      "loss at 41th iteration 0.6185024149215907\n",
      "loss at 42th iteration 0.6264560902231736\n",
      "loss at 43th iteration 0.610947708315027\n",
      "loss at 44th iteration 0.617559239357495\n",
      "loss at 45th iteration 0.6243921903739027\n",
      "loss at 46th iteration 0.6308498716199841\n",
      "loss at 47th iteration 0.6140731287727612\n",
      "loss at 48th iteration 0.6000528825340182\n",
      "loss at 49th iteration 0.5874470270692372\n",
      "loss at 50th iteration 0.5934913715378013\n",
      "loss at 51th iteration 0.5997489149513308\n",
      "loss at 52th iteration 0.6059185428932488\n",
      "loss at 53th iteration 0.6121131511397139\n",
      "loss at 54th iteration 0.5989005380203993\n",
      "loss at 55th iteration 0.6050950867850926\n",
      "loss at 56th iteration 0.5931344511984362\n",
      "loss at 57th iteration 0.5825042781892382\n",
      "loss at 58th iteration 0.587959785715236\n",
      "loss at 59th iteration 0.593471266170337\n",
      "loss at 60th iteration 0.5831399222729087\n",
      "loss at 61th iteration 0.5885490073506331\n",
      "loss at 62th iteration 0.5941427332898331\n",
      "loss at 63th iteration 0.5839361539286524\n",
      "loss at 64th iteration 0.5892671589085405\n",
      "loss at 65th iteration 0.5800520723824039\n",
      "loss at 66th iteration 0.5719492804851964\n",
      "loss at 67th iteration 0.5767082183471304\n",
      "loss at 68th iteration 0.5695510105358604\n",
      "loss at 69th iteration 0.574050261794595\n",
      "loss at 70th iteration 0.5671346688659403\n",
      "loss at 71th iteration 0.5713650973168997\n",
      "loss at 72th iteration 0.575697574993482\n",
      "loss at 73th iteration 0.5802162377097632\n",
      "loss at 74th iteration 0.5849807435145334\n",
      "loss at 75th iteration 0.5764298808164738\n",
      "loss at 76th iteration 0.5691022743024157\n",
      "loss at 77th iteration 0.5735141363439218\n",
      "loss at 78th iteration 0.5777388178859593\n",
      "loss at 79th iteration 0.57065303269718\n",
      "loss at 80th iteration 0.5641788532587539\n",
      "loss at 81th iteration 0.5587963564668483\n",
      "loss at 82th iteration 0.5624241896341895\n",
      "loss at 83th iteration 0.566340661102922\n",
      "loss at 84th iteration 0.5606585808294984\n",
      "loss at 85th iteration 0.5559139244911051\n",
      "loss at 86th iteration 0.5520769595454637\n",
      "loss at 87th iteration 0.5546939258803525\n",
      "loss at 88th iteration 0.5510037570336257\n",
      "loss at 89th iteration 0.5537933576553591\n",
      "loss at 90th iteration 0.5504117257078147\n",
      "loss at 91th iteration 0.5476222021078335\n",
      "loss at 92th iteration 0.5453734131390795\n",
      "loss at 93th iteration 0.5436282604131332\n",
      "loss at 94th iteration 0.5425689910655508\n",
      "loss at 95th iteration 0.5434523776293009\n",
      "loss at 96th iteration 0.5447403128263028\n",
      "loss at 97th iteration 0.5463893015742238\n",
      "loss at 98th iteration 0.544480838432548\n",
      "loss at 99th iteration 0.5431802801327101\n",
      "loss at 100th iteration 0.5422926803923456\n",
      "loss at 101th iteration 0.5431939481873433\n",
      "loss at 102th iteration 0.5444869991816508\n",
      "loss at 103th iteration 0.5457418034153395\n",
      "loss at 104th iteration 0.5476276953227943\n",
      "loss at 105th iteration 0.549810444692412\n",
      "loss at 106th iteration 0.5519283506356172\n",
      "loss at 107th iteration 0.5542550609573224\n",
      "loss at 108th iteration 0.5511519128965982\n",
      "loss at 109th iteration 0.553750308861209\n",
      "loss at 110th iteration 0.5561125375510907\n",
      "loss at 111th iteration 0.5590661475830541\n",
      "loss at 112th iteration 0.5618633446059461\n",
      "loss at 113th iteration 0.5574294726222506\n",
      "loss at 114th iteration 0.5537622241270812\n",
      "loss at 115th iteration 0.5561888263500117\n",
      "loss at 116th iteration 0.5589089089045494\n",
      "loss at 117th iteration 0.5618292829903954\n",
      "loss at 118th iteration 0.5576033109691612\n",
      "loss at 119th iteration 0.560374510695995\n",
      "loss at 120th iteration 0.5633089340347549\n",
      "loss at 121th iteration 0.5590207127822764\n",
      "loss at 122th iteration 0.5553118730082924\n",
      "loss at 123th iteration 0.557834183207765\n",
      "loss at 124th iteration 0.560651535566176\n",
      "loss at 125th iteration 0.5636867385064562\n",
      "loss at 126th iteration 0.5596228908147075\n",
      "loss at 127th iteration 0.5559496298668292\n",
      "loss at 128th iteration 0.5528693678842054\n",
      "loss at 129th iteration 0.5553254563649469\n",
      "loss at 130th iteration 0.5577958594893212\n",
      "loss at 131th iteration 0.5547105751444207\n",
      "loss at 132th iteration 0.5572528553518434\n",
      "loss at 133th iteration 0.553777608345638\n",
      "loss at 134th iteration 0.5506980008004195\n",
      "loss at 135th iteration 0.5483252727424421\n",
      "loss at 136th iteration 0.5465350391608776\n",
      "loss at 137th iteration 0.5482630413456162\n",
      "loss at 138th iteration 0.5502348407162444\n",
      "loss at 139th iteration 0.5524250481501509\n",
      "loss at 140th iteration 0.5496021702710536\n",
      "loss at 141th iteration 0.5474605239395833\n",
      "loss at 142th iteration 0.5457711686374891\n",
      "loss at 143th iteration 0.5470701757481412\n",
      "loss at 144th iteration 0.5483081530634286\n",
      "loss at 145th iteration 0.5463410834858555\n",
      "loss at 146th iteration 0.544878519751922\n",
      "loss at 147th iteration 0.5459471399454712\n",
      "loss at 148th iteration 0.5447082423205971\n",
      "loss at 149th iteration 0.5458510886671991\n",
      "loss at 150th iteration 0.5446345163856559\n",
      "loss at 151th iteration 0.5455619784949423\n",
      "loss at 152th iteration 0.5443738385889901\n",
      "loss at 153th iteration 0.5454362987078052\n",
      "loss at 154th iteration 0.5469690605091255\n",
      "loss at 155th iteration 0.545185601435874\n",
      "loss at 156th iteration 0.5438045141484333\n",
      "loss at 157th iteration 0.5449711748219317\n",
      "loss at 158th iteration 0.5437438914419349\n",
      "loss at 159th iteration 0.5430115495932918\n",
      "loss at 160th iteration 0.5424028010992277\n",
      "loss at 161th iteration 0.5420061862029499\n",
      "loss at 162th iteration 0.5418095534243591\n",
      "loss at 163th iteration 0.5419326169539875\n",
      "loss at 164th iteration 0.542348665329755\n",
      "loss at 165th iteration 0.54300851962366\n",
      "loss at 166th iteration 0.5422750453231966\n",
      "loss at 167th iteration 0.5418532809449397\n",
      "loss at 168th iteration 0.5416813836038441\n",
      "loss at 169th iteration 0.5417869435983103\n",
      "loss at 170th iteration 0.5421560970109843\n",
      "loss at 171th iteration 0.5419500844466363\n",
      "loss at 172th iteration 0.5417962781882822\n",
      "loss at 173th iteration 0.5417867102717808\n",
      "loss at 174th iteration 0.542026357835932\n",
      "loss at 175th iteration 0.5425540720402621\n",
      "loss at 176th iteration 0.542003496000692\n",
      "loss at 177th iteration 0.54247813962686\n",
      "loss at 178th iteration 0.5431831509564082\n",
      "loss at 179th iteration 0.5440066873058611\n",
      "loss at 180th iteration 0.5430444061008017\n",
      "loss at 181th iteration 0.5438807667110044\n",
      "loss at 182th iteration 0.5449086442303104\n",
      "loss at 183th iteration 0.543740766242746\n",
      "loss at 184th iteration 0.5447301586357893\n",
      "loss at 185th iteration 0.5458705217220666\n",
      "loss at 186th iteration 0.5471995034341002\n",
      "loss at 187th iteration 0.5486958362848192\n",
      "loss at 188th iteration 0.5468575294079029\n",
      "loss at 189th iteration 0.5482582710204493\n",
      "loss at 190th iteration 0.5498168260569511\n",
      "loss at 191th iteration 0.5478981943129722\n",
      "loss at 192th iteration 0.5463012898644857\n",
      "loss at 193th iteration 0.5449330421281243\n",
      "loss at 194th iteration 0.5438020781156443\n",
      "loss at 195th iteration 0.5448109807766967\n",
      "loss at 196th iteration 0.5436566695577347\n",
      "loss at 197th iteration 0.5446515201109234\n",
      "loss at 198th iteration 0.5457269894600286\n",
      "loss at 199th iteration 0.5444378999699461\n",
      "0.4813334618983277 0.07323950898393737\n"
     ]
    }
   ],
   "source": [
    "(final_x,final_y)=gradient_descent(data, 200)\n",
    "print(final_x,final_y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486427f",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245862c",
   "metadata": {},
   "source": [
    "1.when we are running stochastic gradient descent with learning rate=0.1 or 0.1/1+t,or 0.1/sqrt(1+t), we are getting close to the original y-min=0,But it will be much more improved if we do more iterations.\n",
    "<br>2.Here are (x,y) that are acheived when we do with different learning rates \n",
    "<br>eta=0.1, the final (x_min,y_min)=(0.46,0.18), here we are oscillating on both sides of minimum rather than going more precise since learning rate is same after many iterations also.\n",
    "<br>eta=0.1/1+t, the final (x_min,y_min)=(0.65,0.12),here learning looks good but update going little slow compared to below.\n",
    "<br>eta=0.1/sqrt(1+t), the final (x_min,y_min)=(0.48,0.07), here learning looks optimal and giving better result.\n",
    "<br>\n",
    "<br> we are getting very close or better results when we are doing with eta=0.1/sqrt(1+t) majority of time.All of the methods are really close and since it is randomized some times other ones are performing better.If we had taken more iterations we could compare much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcb28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
