{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4a4310",
   "metadata": {},
   "source": [
    "**Runnning of Gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a12215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start, learning_rate,iterations, data):\n",
    "    weight = start\n",
    "    print(\"Starting Loss:\", loss(weight, data))\n",
    "    for i in range(iterations):\n",
    "        weight =weight-learning_rate * gradient(weight, data) \n",
    "        print(\"loss at \"+str(i)+\"th iteration \" +str(loss(weight, data)))\n",
    "    return weight\n",
    "\n",
    "def gradient(w, data):\n",
    "    grad = 0\n",
    "    for point in data:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        grad += (-1 * x * y) * math.exp(-1 * x * y * w) / (1+math.exp(-1 * x * y * w))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7ff1a",
   "metadata": {},
   "source": [
    "**Finding Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebbf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, data):\n",
    "    loss = 0\n",
    "    for point in data:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        loss += math.log(1 + math.exp(-1 * x * y * w))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe54936",
   "metadata": {},
   "source": [
    "**Creating Normal Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d357ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def normal_data():\n",
    "    data = []\n",
    "    for i in range(-50, 0):\n",
    "        point = (i, -1)\n",
    "        data.append(point)\n",
    "        point = (-i, 1)\n",
    "        data.append(point)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4d139",
   "metadata": {},
   "source": [
    "**Calling the Gradient Descent for Normal Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a3aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Loss: 2551.0351202142683\n",
      "loss at 0th iteration 1901.7246598540137\n",
      "loss at 1th iteration 1253.3340798105403\n",
      "loss at 2th iteration 608.3979926654177\n",
      "loss at 3th iteration 51.67631439998536\n",
      "loss at 4th iteration 14.161717351746782\n",
      "loss at 5th iteration 12.616098037073103\n",
      "loss at 6th iteration 11.576297054996056\n",
      "loss at 7th iteration 10.807218608693677\n",
      "loss at 8th iteration 10.20472390853849\n",
      "loss at 9th iteration 9.714048698699553\n",
      "loss at 10th iteration 9.303071092254\n",
      "loss at 11th iteration 8.95145329911838\n",
      "loss at 12th iteration 8.645565832444527\n",
      "loss at 13th iteration 8.37586385192463\n",
      "loss at 14th iteration 8.135423839982822\n",
      "loss at 15th iteration 7.919077149636532\n",
      "loss at 16th iteration 7.7228715373997865\n",
      "loss at 17th iteration 7.543722865512494\n",
      "loss at 18th iteration 7.3791820943069855\n",
      "loss at 19th iteration 7.227274853060722\n",
      "loss at 20th iteration 7.086388200597225\n",
      "loss at 21th iteration 6.955188939454656\n",
      "loss at 22th iteration 6.8325635523188195\n",
      "loss at 23th iteration 6.717573279818634\n",
      "loss at 24th iteration 6.609420007844315\n",
      "loss at 25th iteration 6.50742000642221\n",
      "loss at 26th iteration 6.410983461179537\n",
      "loss at 27th iteration 6.319598339148216\n",
      "loss at 28th iteration 6.232817539735166\n",
      "loss at 29th iteration 6.150248565101202\n",
      "loss at 30th iteration 6.071545143656074\n",
      "loss at 31th iteration 5.996400382798342\n",
      "loss at 32th iteration 5.9245411300777695\n",
      "loss at 33th iteration 5.855723297438265\n",
      "loss at 34th iteration 5.78972795911813\n",
      "loss at 35th iteration 5.7263580756495305\n",
      "loss at 36th iteration 5.665435728053033\n",
      "loss at 37th iteration 5.606799770476124\n",
      "loss at 38th iteration 5.550303828113684\n",
      "loss at 39th iteration 5.495814581670925\n",
      "loss at 40th iteration 5.44321029090378\n",
      "loss at 41th iteration 5.392379518649097\n",
      "loss at 42th iteration 5.34322002379336\n",
      "loss at 43th iteration 5.2956377972419\n",
      "loss at 44th iteration 5.249546219455271\n",
      "loss at 45th iteration 5.204865321755327\n",
      "loss at 46th iteration 5.161521136554115\n",
      "loss at 47th iteration 5.119445124065276\n",
      "loss at 48th iteration 5.078573665030572\n",
      "loss at 49th iteration 5.038847610618682\n",
      "loss at 50th iteration 5.000211881997331\n",
      "loss at 51th iteration 4.96261511319629\n",
      "loss at 52th iteration 4.926009331809903\n",
      "loss at 53th iteration 4.890349672867682\n",
      "loss at 54th iteration 4.8555941218568215\n",
      "loss at 55th iteration 4.821703283433281\n",
      "loss at 56th iteration 4.78864017282609\n",
      "loss at 57th iteration 4.756370027336772\n",
      "loss at 58th iteration 4.724860135674268\n",
      "loss at 59th iteration 4.694079683155045\n",
      "loss at 60th iteration 4.663999611045796\n",
      "loss at 61th iteration 4.6345924885393215\n",
      "loss at 62th iteration 4.605832396037554\n",
      "loss at 63th iteration 4.577694818574475\n",
      "loss at 64th iteration 4.550156548348859\n",
      "loss at 65th iteration 4.523195595456251\n",
      "loss at 66th iteration 4.496791106013269\n",
      "loss at 67th iteration 4.4709232869580005\n",
      "loss at 68th iteration 4.445573336889347\n",
      "loss at 69th iteration 4.420723382377693\n",
      "loss at 70th iteration 4.396356419240028\n",
      "loss at 71th iteration 4.372456258326444\n",
      "loss at 72th iteration 4.349007475411953\n",
      "loss at 73th iteration 4.325995364829499\n",
      "loss at 74th iteration 4.303405896516809\n",
      "loss at 75th iteration 4.281225676182509\n",
      "loss at 76th iteration 4.2594419083259965\n",
      "loss at 77th iteration 4.238042361871341\n",
      "loss at 78th iteration 4.217015338198554\n",
      "loss at 79th iteration 4.196349641376096\n",
      "loss at 80th iteration 4.176034550416812\n",
      "loss at 81th iteration 4.156059793395953\n",
      "loss at 82th iteration 4.1364155232846205\n",
      "loss at 83th iteration 4.117092295365225\n",
      "loss at 84th iteration 4.0980810461073975\n",
      "loss at 85th iteration 4.079373073393493\n",
      "loss at 86th iteration 4.060960017992448\n",
      "loss at 87th iteration 4.042833846189467\n",
      "loss at 88th iteration 4.024986833486862\n",
      "loss at 89th iteration 4.007411549298416\n",
      "loss at 90th iteration 3.990100842566217\n",
      "loss at 91th iteration 3.9730478282345674\n",
      "loss at 92th iteration 3.9562458745209863\n",
      "loss at 93th iteration 3.9396885909291637\n",
      "loss at 94th iteration 3.9233698169529285\n",
      "loss at 95th iteration 3.907283611424536\n",
      "loss at 96th iteration 3.891424242463968\n",
      "loss at 97th iteration 3.875786177989464\n",
      "loss at 98th iteration 3.860364076752374\n",
      "loss at 99th iteration 3.8451527798623033\n",
      "final weights: 0.36490098099065155\n"
     ]
    }
   ],
   "source": [
    "norm_data = normal_data()\n",
    "w = gradient_descent(-1,0.0001,100,norm_data)\n",
    "print(\"final weights:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ac393",
   "metadata": {},
   "source": [
    "**Creating Corrupt Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47afa3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_data():\n",
    "    data = []\n",
    "    for i in range(-50, -45):\n",
    "        point = (i, 1)\n",
    "        data.append(point)\n",
    "        point = (-i, -1)\n",
    "        data.append(point)\n",
    "    for i in range(-45, 0):\n",
    "        point = (i, -1)\n",
    "        data.append(point)\n",
    "        point = (-i, 1)\n",
    "        data.append(point)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb3f89",
   "metadata": {},
   "source": [
    "**Calling the Gradient Descent for Corrupt Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d06aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Loss: 2071.0351202142688\n",
      "loss at 0th iteration 1643.280110003809\n",
      "loss at 1th iteration 1216.0280714839787\n",
      "loss at 2th iteration 790.0188510127545\n",
      "loss at 3th iteration 368.9751391790776\n",
      "loss at 4th iteration 55.18977048675905\n",
      "loss at 5th iteration 53.23083146737823\n",
      "loss at 6th iteration 52.99046906395333\n",
      "loss at 7th iteration 52.99041643081455\n",
      "loss at 8th iteration 52.990415880131096\n",
      "loss at 9th iteration 52.99041587452638\n",
      "loss at 10th iteration 52.99041587446917\n",
      "loss at 11th iteration 52.99041587446855\n",
      "loss at 12th iteration 52.99041587446856\n",
      "loss at 13th iteration 52.99041587446857\n",
      "loss at 14th iteration 52.99041587446857\n",
      "loss at 15th iteration 52.990415874468574\n",
      "loss at 16th iteration 52.99041587446857\n",
      "loss at 17th iteration 52.99041587446858\n",
      "loss at 18th iteration 52.99041587446861\n",
      "loss at 19th iteration 52.99041587446861\n",
      "loss at 20th iteration 52.990415874468596\n",
      "loss at 21th iteration 52.990415874468596\n",
      "loss at 22th iteration 52.990415874468596\n",
      "loss at 23th iteration 52.990415874468596\n",
      "loss at 24th iteration 52.990415874468596\n",
      "loss at 25th iteration 52.990415874468596\n",
      "loss at 26th iteration 52.990415874468596\n",
      "loss at 27th iteration 52.990415874468596\n",
      "loss at 28th iteration 52.990415874468596\n",
      "loss at 29th iteration 52.990415874468596\n",
      "loss at 30th iteration 52.990415874468596\n",
      "loss at 31th iteration 52.990415874468596\n",
      "loss at 32th iteration 52.990415874468596\n",
      "loss at 33th iteration 52.990415874468596\n",
      "loss at 34th iteration 52.990415874468596\n",
      "loss at 35th iteration 52.990415874468596\n",
      "loss at 36th iteration 52.990415874468596\n",
      "loss at 37th iteration 52.990415874468596\n",
      "loss at 38th iteration 52.990415874468596\n",
      "loss at 39th iteration 52.990415874468596\n",
      "loss at 40th iteration 52.990415874468596\n",
      "loss at 41th iteration 52.990415874468596\n",
      "loss at 42th iteration 52.990415874468596\n",
      "loss at 43th iteration 52.990415874468596\n",
      "loss at 44th iteration 52.990415874468596\n",
      "loss at 45th iteration 52.990415874468596\n",
      "loss at 46th iteration 52.990415874468596\n",
      "loss at 47th iteration 52.990415874468596\n",
      "loss at 48th iteration 52.990415874468596\n",
      "loss at 49th iteration 52.990415874468596\n",
      "loss at 50th iteration 52.990415874468596\n",
      "loss at 51th iteration 52.990415874468596\n",
      "loss at 52th iteration 52.990415874468596\n",
      "loss at 53th iteration 52.990415874468596\n",
      "loss at 54th iteration 52.990415874468596\n",
      "loss at 55th iteration 52.990415874468596\n",
      "loss at 56th iteration 52.990415874468596\n",
      "loss at 57th iteration 52.990415874468596\n",
      "loss at 58th iteration 52.990415874468596\n",
      "loss at 59th iteration 52.990415874468596\n",
      "loss at 60th iteration 52.990415874468596\n",
      "loss at 61th iteration 52.990415874468596\n",
      "loss at 62th iteration 52.990415874468596\n",
      "loss at 63th iteration 52.990415874468596\n",
      "loss at 64th iteration 52.990415874468596\n",
      "loss at 65th iteration 52.990415874468596\n",
      "loss at 66th iteration 52.990415874468596\n",
      "loss at 67th iteration 52.990415874468596\n",
      "loss at 68th iteration 52.990415874468596\n",
      "loss at 69th iteration 52.990415874468596\n",
      "loss at 70th iteration 52.990415874468596\n",
      "loss at 71th iteration 52.990415874468596\n",
      "loss at 72th iteration 52.990415874468596\n",
      "loss at 73th iteration 52.990415874468596\n",
      "loss at 74th iteration 52.990415874468596\n",
      "loss at 75th iteration 52.990415874468596\n",
      "loss at 76th iteration 52.990415874468596\n",
      "loss at 77th iteration 52.990415874468596\n",
      "loss at 78th iteration 52.990415874468596\n",
      "loss at 79th iteration 52.990415874468596\n",
      "loss at 80th iteration 52.990415874468596\n",
      "loss at 81th iteration 52.990415874468596\n",
      "loss at 82th iteration 52.990415874468596\n",
      "loss at 83th iteration 52.990415874468596\n",
      "loss at 84th iteration 52.990415874468596\n",
      "loss at 85th iteration 52.990415874468596\n",
      "loss at 86th iteration 52.990415874468596\n",
      "loss at 87th iteration 52.990415874468596\n",
      "loss at 88th iteration 52.990415874468596\n",
      "loss at 89th iteration 52.990415874468596\n",
      "loss at 90th iteration 52.990415874468596\n",
      "loss at 91th iteration 52.990415874468596\n",
      "loss at 92th iteration 52.990415874468596\n",
      "loss at 93th iteration 52.990415874468596\n",
      "loss at 94th iteration 52.990415874468596\n",
      "loss at 95th iteration 52.990415874468596\n",
      "loss at 96th iteration 52.990415874468596\n",
      "loss at 97th iteration 52.990415874468596\n",
      "loss at 98th iteration 52.990415874468596\n",
      "loss at 99th iteration 52.990415874468596\n",
      "final weights: 0.046148698970806555\n"
     ]
    }
   ],
   "source": [
    "curr_data = corrupt_data()\n",
    "w1 = gradient_descent(-1,0.0001,100,curr_data)\n",
    "print(\"final weights:\", w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf217d",
   "metadata": {},
   "source": [
    "# #Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849459d5",
   "metadata": {},
   "source": [
    "<br>1.The loss is decreasing for both the functions really well when we had taken a learning rate =0.0001.if we take more that that it started oscillating between minimum.\n",
    "<br>2. But the final loss of the correct data will tend to 3. where as final loss of the corrupt data will be tending a much larger value 52 as seen above. since some of the data is corrrupt and since our data set is small,we always get more loss compared to the normal where all data is perfectly correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8a644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
